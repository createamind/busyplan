



https://arxiv.org/pdf/1810.05546.pdf.
Uncertainty in Neural Networks: Bayesian Ensembling


---------------------------------------------------
https://arxiv.org/pdf/1812.10539.pdf
Uncertainty Autoencoders: Learning Compressed Representations via Variational Information Maximization




7
uncertainty autoencoders (UAE), a framework for representation learning via variational maximization of mutual information be- tween an input signal and hidden representation. We showed that UAEs are a natural candidate for sta- tistical compressive sensing,




----------------------------------------------
https://arxiv.org/pdf/1812.11971.pdf
Mid-Level Visual Representations Improve  Generalization and Sample Efficiency for Learning Active Tasks
code will release


We test three core hypotheses:
I. if mid-level vision pro- vides an advantage in terms of sample efficiency of learning an active task (answer: yes)
II. if mid-level vision provides an advantage towards generalization to unseen spaces (an- swer: yes)
III. if a fixed mid-level vision feature could suf- fice or a set of features would be essential to support arbi- trary active tasks (answer: a set is essential).

Hypothesis I: Does mid-level vision provide an advantage in terms of sample efficiency when learning an active task?
Hypothesis II: Can mid-level vision features generalize better to unseen spaces?
Hypothesis III: Can a single feature support all arbitrary downstream tasks? Or is a set of features required for that?















paper: sac

two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning

soft actor-critic, an off- policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning frame- work.

actor aims to maxi- mize expected reward while also maximizing en- tropy. That is, to succeed at the task while acting as randomly as possible.

combining off-policy updates with a stable stochastic actor-critic

two major challenges. First, model-free deep RL meth- ods are notoriously expensive in terms of their sample com- plexity. Even relatively simple tasks can require millions of steps of data collection, and complex behaviors with high- dimensional observations might need substantially more. Second, these methods are often brittle with respect to their hyperparameters: learning rates, exploration constants, and other settings must be set carefully for different problem settings to achieve good results.
One cause for the poor sample efficiency of deep RL meth- ods is on-policy learning: some of the most commonly used deep RL algorithms, such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016), require new samples
Unfortunately, the combina- tion of off-policy learning and high-dimensional, nonlinear function approximation with neural networks presents a ma- jor challenge for stability and convergence
importantly, the maximum entropy formulation provides a substantial improvement in exploration and robustness
three key in- gredients: an actor-critic architecture with separate policy and value function networks, an off-policy formulation that enables reuse of previously collected data for efficiency, and entropy maximization to enable stability and exploration.
Actor-critic algorithms are typically derived starting from policy iteration, which alternates between pol- icy evaluation—computing the value function for a policy—
and policy improvement—using the value function to obtain a better policy
policy is referred to as the actor, and the value function as the critic.
ddpg  method can be viewed both as a determinis- tic actor-critic algorithm and an approximate Q-learning
 difficult to stabilize and brittle to hyperpa- rameter settings
Our method instead combines off-policy actor- critic training with a stochastic actor, and further aims to maximize the entropy of this actor with an entropy maxi- mization objective.
Maximum entropy reinforcement learning optimizes poli- cies to maximize both the expected return and the ex- pected entropy of the policy.
3.1
 ρπ(st) and ρπ(st,at) to denote the state and state-action marginals of the trajectory distribution induced by a policy π(at|st).
J(π) = 􏰐 E(st,at)∼ρπ [r(st, at) + αH(π( · |st))] . (1)
t=0
The temperature parameter α determines
This objective has a number of conceptual and practical advantages. First, the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues. Second,the policy can capture multiple modes of near- optimal behavior
multiple ac- tions seem equally attractive, the policy will commit equal probability mass to those actions
4
4.1. Derivation of Soft Policy Iteration
learning optimal maximum entropy policies that alternates between policy evaluation and policy improve- ment in the maximum entropy framework.
show that soft policy iteration converges to the optimal policy within a set of policies which might correspond, for instance, to a set of parameterized densities
policy evaluation step of soft policy iteration, we wish to compute the value of a policy π according to the maximum entropy objective in Equation 1.
For a fixed policy, the soft Q-value can be computed iteratively, starting from any function Q : S × A → R and repeatedly applying a modified Bellman backup operator T π given by
Lemma 1 (Soft Policy Evaluation). Consider the soft Bell- man backup operator T π in Equation 2 and a mapping Q0 :S×A→Rwith|A|<∞,anddefineQk+1 =TπQk. Then the sequence Qk will converge to the soft Q-value of π as k → ∞.

policy improvement step, we update the policy to- wards the exponential of the new Q-function.
in the policy improvement step, for each state, we update the policy according to

paper sac2  https://bair.berkeley.edu/blog/2018/12/14/sac/

chapter 5:


paper: sac in real world ;  https://bair.berkeley.edu/blog/2018/12/14/sac/

Desired Features for Deep RL for Real Robots
What makes an ideal deep RL algorithm for real-world systems? Real-world experimentation brings additional challenges, such as constant interruptions in the data stream, requirement for a low-latency inference and smooth exploration to avoid mechanical wear and tear on the robot, which set additional requirement for both the algorithm and also the implementation of the algorithm.

Regarding the algorithm, several properties are desirable:

Sample Efficiency. Learning skills in the real world can take a substantial amount of time. Prototyping a new task takes several trials, and the total time required to learn a new skill quickly adds up. Thus good sample complexity is the first prerequisite for successful skill acquisition.
No Sensitive Hyperparameters. In the real world, we want to avoid parameter tuning for the obvious reason. Maximum entropy RL provides a robust framework that minimizes the need for hyperparameter tuning.
Off-Policy Learning. An algorithm is off-policy if we can reuse data collected for another task. In a typical scenario, we need to adjust parameters and shape the reward function when prototyping a new task, and use of an off-policy algorithm allows reusing the already collected data.
Soft actor-critic (SAC), described below, is an off-policy model-free deep RL algorithm that is well aligned with these requirements. In particular, we show that it is sample efficient enough to solve real-world robot tasks in only a handful of hours, robust to hyperparameters and works on a variety of simulated environments with a single set of hyperparameters.

In addition to the desired algorithmic properties, experimentation in the real-world sets additional requirements for the implementation. Our release supports many of these features that we have found crucial when learning with real robots, perhaps the most importantly:

Asynchronous Sampling. Inference needs to be fast to minimize delay in the control loop, and we typically want to keep training during the environment resets too. Therefore, data sampling and training should run in independent threads or processes.
Stop / Resume Training. When working with real hardware, whatever can go wrong, will go wrong. We should expect constant interruptions in the data stream.
Action smoothing. Typical Gaussian exploration makes the actuators jitter at high frequency, potentially damaging the hardware. Thus temporally correlating the exploration is important.





------

paper: polo

abs：
between local model-based control, global value function learning, and exploration.
local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning
approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions
trajec- tory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation
exploration is critical for fast and stable learning of the value function.
2.2Value Function Approximation
The core structure of fitted value iteration considers a collection of states (or a sampling distribution ν), and a parametric value function approximator Vˆθ. Inspired by value iteration, fitted value iteration updates the parameters as:
success of this overall procedure depends critically on at least two components: the capacity of the function approximator and the sampling distribution ν.
lemma 1
performance of πˆ degrades with a dependence on effective problem horizon determined by γ.
understood as the policy paying a price of ε at every timestep.
arg max operation in πˆ could inadvertently exploit approximation errors to produce a poor policy.
performance of fitted value iteration based methods also rely critically on the sampling distribu- tion to propagate global information
specify good sampling distributions using apriori knowledge of where the op- timal policy should visit (e.g. based on demonstration data). However, in the setting where an agent is dropped into a new world or task, automatically generating such sampling distributions may be difficult, and is analogous to the problem of exploration.
2.3
MPC looks forward
1In this work, we use the terms trajectory optimization and MPC interchangeably 3
only H steps, it is ultimately a local method unless coupled with a value function that propagates global information. In addition, we also provide intuitions for why MPC may help accelerate the learning of value functions.
Impact of approximation errors in the value function
Lemma 2. For all MDPs and β, the performance of the MPC policy eq. (3) can be bounded as:
This suggests that MPC (H > 1) is less susceptible to approximation errors than greedy action selection
MPC can benifit from broad knowledge of which parts of the state space are more favorable.
Accelerating convergence of the value function
MPC can also enable faster con- vergence of the value function approximation. To motivate this, consider the H-step Bellman opera-
tor: Intuitively, BH allows for propagation of global information for H steps, thereby accelerating the convergence due to faster mixing.
Note that one way to realize BH is to simply apply B H times, with each step providing a contraction by γ
MPC based on local dynamic programming methods [14, 15] provide an efficient way to approximately realize BH , which can be used to accelerate and stabilize value function learning.
2.4 planning to explore
explore the relevant parts of the state space is critical for the convergence of many RL algorithms
by using MPC, the agent can explore in the space of trajectories. The agent can consider a hypothesis of potential reward regions in the state space, and then execute the optimal trajectory conditioned on this belief, resulting in a temporally coordinated sequence of actions. By executing such coordinated actions, the agent can cover the state space more rapidly and intentionally, and avoid back and forth wandering that can slow down the learning. We demonstrate this effect empirically in Section 3.1.


??
??
??
2.4 Planning to Explore
The ability of an agent to explore the relevant parts of the state space is critical for the convergence of many RL algorithms. Typical exploration strategies like ε-greedy and Boltzmann take exploratory actions with some probability on a per time-step basis. Instead, by using MPC, the agent can explore in the space of trajectories. The agent can consider a hypothesis of potential reward regions in the state space, and then execute the optimal trajectory conditioned on this belief, resulting in a temporally coordinated sequence of actions. By executing such coordinated actions, the agent can cover the state space more rapidly and intentionally, and avoid back and forth wandering that can slow down the learning. We demonstrate this effect empirically in Section 3.1.
To generate the hypothesis of potentially rewarding regions, we take a Bayesian view and approxi- mately track a posterior over value functions. Consider a motivating setting of regression, where we have a parametric function approximator fθ with prior P(θ). The dataset consists of input-output pairs: D = (xi, yi)ni=1, and we wish to approximate P(θ|D). In the Bayesian linear regression set- ting with Gaussian prior and noise models, the solution to the following problem generates samples from the posterior [16]:
σ2
argmin||y ̃i −fθ ̃(xi)−fθ(xi)||2 + ||θ||2 (5)
where y ̃i ∼ N(yi,σ2) is a noisy version of the target and θ ̃ ∼ P(θ) is a sample from the prior. Based on this, Osband et al. [16] demonstrate the benefits of uncertainty estimation for exploration. Similarly, we use this procedure to obtain samples from the posterior for value function approxima- tion, and utilize them for temporally coordinated action selection using MPC. We consider K value function approximators Vˆθ with parameters θ1, θ2, . . . θK independently trained based on eq. (5). We consider the softmax of the different samples as the value at a state:
􏰍K􏰎
Vˆ(s)=log 􏰁exp􏰆κVˆθk(s)􏰇 . (6) k=1
Since the log-sum-exp function approximates mean + variance for small κ > 0, this procedure encourages the agent to additionally explore parts of the state space where the disagreement between the function approximators is large. This corresponds to the broad notion of optimism in the face of uncertainty [17] which has been successful in a number of applications [18, 19].



2.5  Final Algorithm
 To summarize, POLO utilizes a global value function approximation scheme, a local trajectory optimization subroutine, and an optimistic exploration scheme. POLO operates as follows: when acting in the world, the agent uses the internal model and always picks the optimal action suggested by MPC. Exploration is implicitly handled by tracking the value function uncertainties and the optimistic evaluation, as specified in eq. (5) and (6). All the experience (visited states) from the world are stored into a replay buffer D, with old experiences discarded if the buffer becomes full. After every Z steps of acting in the world and collecting experience, the value functions are updated by: (a) constructing the targets according to eq. (7); (b) performing regression using the randomized prior scheme using eq. (5) where fθ corresponds to the value function approximator. For state si in the buffer and value network k with parameters θk, the targets are constructed as:


3 Empirical Results and Discussion
Through empirical evaluation, we wish to answer the following questions:
1. Does trajectory optimization in conjunction with uncertainty estimation in value function approximation result in temporally coordinated exploration strategies?
2. Can the use of an approximate value function help reduce the planning horizon for MPC? 3. Does trajectory optimization enable faster and more stable value function learning?




4.2
However, such approaches critically do not have the element of planning to explore; thus the agent may not actually reach regions of high predicted reward because it does not know how to get there. Our work is perhaps closest to the E3 framework of Kearns & Singh [50], which considers altered MDPs with different reward functions, and executes the optimal action under that MDP. However solving these altered MDPs is expensive and their solution is quickly discarded. MPC on the other hand can quickly solve for local instance- specific solutions in these MDPs.






------

paper: IMPROVING MODEL-BASED CONTROL AND ACTIVE EXPLORATION WITH RECONSTRUCTION UNCERTAINTY OPTIMIZATION

2.1
A widely used model-based control algorithm is Model Predictive Control. It consists of sampling a series of possible
future actions sequences, predicting the cumulative reward of each based on the model, choosing the trajectory with the highest predicted reward and then executing only the first action.


---

Randomized Prior Functions for Deep Reinforcement Learning

highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable ‘prior’ network to each ensemble member. We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.
1
elementary decision theory shows that the only admissible decision rules are Bayesian [12, 71]. Colloquially, this means that any decision rule that is not Bayesian can be improved (or even exploited) by some Bayesian alternative
In this paper we highlight the surprising fact that many of these well-cited and popular methods for uncertainty estimation in deep learning can be poor choices for sequential decision problems
serious shortcoming that can lead to arbitrarily poor performance. We support our claims by a series of simple lemmas for simple environments, together with experimental evidence in more complex settings.
However, these ensemble-based approaches to uncertainty quantification approximate a ‘posterior’ without an effective methodology to inject a ‘prior’. This can be a crucial shortcoming in sequential decision problems.

initialized together with a random but fixed prior function.
Learning/optimization is performed so that this sum (network plus prior) minimizes training loss
However, in regions of the space with little or no training data, their predictions will be determined by the generalization of their networks and priors.
Surprisingly, we show that this approach is equivalent to exact Bayesian inference for the special case of Gaussian linear models. Following on from this ‘sanity check’, we present a series of simple experiments designed to extend this intuition to deep learning.
We show that many of the most popular approaches for uncertainty estimation in deep RL do not pass these sanity checks, and crystallize these shortcomings in a series of lemmas and small examples. We demonstrate that our simple modification can facilitate aspiration in difficult tasks where previous approaches for deep RL fail. We believe that this work presents a simple and practical approach to encoding prior knowledge with deep reinforcement learning.
2
scale and scope of problems that might be approached through deep RL is vast, but there are three key aspects an efficient (and general) agent must address [63]:
1. Generalization: be able to learn from data it collects.
2. Exploration: prioritize the best experiences to learn from.
3. Long-term consequences: consider external effects beyond a single time step.
this paper we focus on the importance of some form of ‘prior’ mechanism for efficient exploration
If an agent has never seen a reward then it is essential that some other form of aspiration, motivation, drive or curiosity direct its learning.
We call this type of drive a ‘prior’ effect, since it does not come from the observed data, but are ambivalent as to whether this effect is philosophically ‘Bayesian’. Agents that do not have this prior drive will be left floundering aimlessly and thus may require exponentially large amounts of data in order to learn even simple problems [27].
A collection of definitions of intelligence. Frontiers in
Artificial Intelligence and applications, 157:17, 2007.

公众号截图发文章了


2
The scale and scope of problems that might be approached through deep RL is vast, but there are three key aspects an efficient (and general) agent must address [63]:
1. Generalization: be able to learn from data it collects.
2. Exploration: prioritize the best experiences to learn from.
3. Long-term consequences: consider external effects beyond a single time step.
In this paper we focus on the importance of some form of ‘prior’ mechanism for efficient exploration. As a motivating example we consider a sparse reward task where random actions are very unlikely to ever see a reward. If an agent has never seen a reward then it is essential that some other form of aspiration, motivation, drive or curiosity direct its learning. We call this type of drive a ‘prior’ effect, since it does not come from the observed data, but are ambivalent as to whether this effect is philosophically ‘Bayesian’. Agents that do not have this prior drive will be left floundering aimlessly and thus may require exponentially large amounts of data in order to learn even simple problems [27].
To solve a specific task, it can be possible to attain superhuman performance without significant prior mechanism [42, 41]. However, if our goal is artificial general intelligence, then it is disconcerting that our best agents can perform very poorly even in simple problems [33, 39]. One potentially general approach to decision making is given by the Thompson sampling heuristic1: ‘randomly take action according to the probability you believe it is the optimal action’ [68].




5 Conclusion
This paper highlights the importance of uncertainty estimates in deep RL, the need for an effective ‘prior’ mechanism, and its potential benefits towards efficient exploration. We present some alarming shortcomings of existing methods and suggest bootstrapped ensembles with randomized prior functions as a simple, practical alternative. We support our claims through an analysis of this method in the linear setting, together with a series of simple experiments designed to highlight the key issues. Our work leaves several open questions. What kinds of prior functions are appropriate for deep RL? Can they be optimized or ‘meta-learned’? Can we distill the ensemble process to a single network? We hope this work helps to inspire solutions to these problems, and also build connections between the theory of efficient learning and practical algorithms for deep reinforcement learning.
C.6 Summary
Table 1: Important issues in posterior approximations for deep reinforcement learning.

https://sites.google.com/view/randomized-prior-nips-2018/
https://colab.research.google.com/drive/1hOZeHjSdoel_-UoeLfg1aRC4uV_6gX1g







3 未来的速度取决于当前场景+当前动作= 下一步的速度和下一步的场景 动作condtion的视频生成 价值很大，验证成功很有用，人的很多决策动作认知都是基于condition，
不同的动作产生不同的结果，就是 vid+action+pred-vid ; 视频+动作+生成的视频 3元判断 vid+action2vid  





2 速度和动作的预测是从endoer中间值还是从预测的最终视频呢？ 中间encoder值处理到低维度 fcn  or drfl ，低纬度后再rnn或CNN来 生成序列
如果从encoder需要resnet结构？unet合适，unet的filter可以从不同层的filter 分别选择？ 如果是学习的最终视频那么unet结构也可以使用来训练生成视频，
视频生成：unet，resnet； 中间结果的Encoder+fcn；
4 强化学习找到视频encoder的某几个filter跟速度预测最相关，这几个filter送入rnn或cnn去预测速度。
5 改造unet可以输出中间encoder值，unet可以多个目标输出，多分支多目标


-------2 pre-train ： ------只训练序列功能无视频生成，预训练序列；  -----2.1 序列和视频生成功能都有，但是开始只训练视频生成功能，等视频生成效果不错了再训练序列生成，
现在从视频b学习seqb，但是视频一直学习的还是A，所以应该学习seqA，或直接先学习seqA，或fix weight train 先训练视频，再训练seq
3 直接unet pretrai video，及video生成效果好了，再生成序列，发现视频生成开始是生成A自己的视频，然后才学习要预测的B的视频。所以可以先进行视频的生成训练，然后进行序列的生成。



G改进：
1.2 视频学习早期还是学习的inputA，所以序列预测增加一个一共两个序列学习，即学习序列seqA，学习序列seqB，看看再训练早期和后期两个序列分别的学习效果！ 实现前沟通确认。
-----验证效果不好，也许是能力不够----1.2.1 序列学习可以从rnn改为cnn，因为我们的速度动作角度都是连续的，可以增加cnn的序列学习模块。cnn 的seq G

D改进   判别器：stat action   reward
2  vid+action+pred-vid ;
2.1 vid+pred-action+vid ;
2.2 vid+reward+pred-action+vid ; vid+reward+action+pred-vid vid+reward+pred-action+pred-vid



?  1 cnn 后fcn 再进rnn 预测动作??， 3 seqgan 哪一行是mc search？ 4 哪一行是强化学习策略梯度相关代码  
5  capvis实现了左右走的视频 6 tcn time contrast network
7 condtions label 是如何加的    10 gps缩略地图和真实地图的映射 就是pix2pix 能学习的。 11 各种condtion gan 如何conditon的  gan集合资料。








inprogress   逐渐细化。  动作值位数太长？？？  1 数据检查，  
数据：数据如何确认合适，数据的合适方式。
单一场景； 多场景；先视频训练再其他功能；    
多场景： 数据目录随机，预测难度也加大了。预测也跟随波动。     ！！一个场景一个场景的训练。数据也连续不随机。
？？ 数据是1条1条的更新的

数据合适以后，是否联调验证跑的效果如何？？无法验证对抗发散是否是训练过度？




-1 图像不动，直接从图像训练序列，realA到 fake seqA
0 图像动作一起训练，监督训练-然后对抗训练  0.1 图像直接进rnn  0.2 图像先encoder 然后filter 进rnn。
1 图像动作一起训练   用seqgan 的强化学习方式训练
2 预训练图像，然后动作单独训练，pretrain  and 对抗训练。或   seqgan强化学习训练     预训练unet or resnet 然后取encoder。
3 预训练图像，cnn的filter 用强化学习选取filter 进行训练。    2 filter drfl like seqgan 交大作者。 3 视频单独的renset训练。。
4 ddpg的动作输出网络参考  强化学习的动作网络。
5 模仿学习的代码论文


动作输出： 1 监督训练，udaciyt nvidia  cnn监督训练方式  2 强化学习方式   3 ？？ 



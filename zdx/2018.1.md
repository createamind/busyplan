

3 未来的速度取决于当前场景+当前动作= 下一步的速度和下一步的场景 动作condtion的视频生成 价值很大，验证成功很有用，人的很多决策动作认知都是基于condition，
不同的动作产生不同的结果，就是 vid+action+pred-vid ; 视频+动作+生成的视频 3元判断 vid+action2vid  




3 seqgan 哪一行是mc search？ 4 哪一行是强化学习策略梯度相关代码  
5  capvis实现了左右走的视频 6 tcn time contrast network
7 condtions label 是如何加的    10 gps缩略地图和真实地图的映射 就是pix2pix 能学习的。 11 各种condtion gan 如何conditon的  gan集合资料。









inprogress   逐渐细化。  动作值位数太长？？？  1 数据检查，  
数据：数据如何确认合适，数据的合适方式。
单一场景； 多场景；先视频训练再其他功能；    
多场景： 数据目录随机，预测难度也加大了。预测也跟随波动。     ！！一个场景一个场景的训练。数据也连续不随机。
？？ 数据是1条1条的更新的

数据合适以后，是否
联调验证跑的效果如何？？无法验证对抗发散是否是训练过度？



G改进：

-1 图像不动，直接从原始图像训练序列，realA到 fake seqA

0 图像动作一起训练，监督训练-然后对抗训练  0.1 图像直接进rnn  0.2 图像先encoder 然后filter 进rnn。   cnn 后fcn 再进rnn
1 图像动作一起训练   用seqgan 的强化学习方式训练
2 预训练图像，然后encoder 的全部filter，即全部encoder结果 来训练动作，即后面再单独单独训练动作，pretrain  and 对抗训练。或   seqgan强化学习训练     预训练unet or resnet 然后取encoder。
3 预训练图像，cnn的filter 用强化学习选取filter 进行训练。    2 filter drfl like seqgan 交大作者。 3 视频单独的renset训练。。
直接unet pretrai video，及video生成效果好了，再生成序列，发现视频生成开始是生成A自己的视频，然后才学习要预测的B的视频。所以可以先进行视频的生成训练，然后进行序列的生成。
----现在从视频b学习seqb，但是视频一直学习的还是A，所以应该学习seqA，或直接先学习seqA，或fix weight train 先训练视频，再训练seq
4 ddpg的动作输出网络参考  强化学习的动作网络。
5 模仿学习的代码论文


动作输出： 1 监督训练，udaciyt（行为clone 监督学习） nvidia  cnn监督训练方式  2 强化学习方式   3 模仿学习  强化对抗学习？？ 



D改进   判别器：stat action   reward   
1 vid fakeaction vid
2  vid+action+pred-vid ;
2.1 vid+pred-action+vid ;
2.2 vid+reward+pred-action+vid ; vid+reward+action+pred-vid vid+reward+pred-action+pred-vid

3 seqA+fakeSeqB; or  FakeseqA; 


视频：1 unet可以各个层的filter一起强化学习选择 2 resnet 3 densenet 
改造unet可以输出中间encoder值，unet可以多个目标输出，多分支多目标




涉及动作的数据集--无人驾驶数据集。 commaai 数据





-------------------------

1  2d conv 可以作为一种实现，3dconv 会提取动作或运动信息有更高层的信息，另外3dconv可以输出一个平均速度，这个平均速度作为一个简单实现 ，强化学习可以选取预训练的3dconv的某几个filter进行学习记忆，不追求和原始数据的完全一致，现在torcs 数据也不完善。

选与动作预测最相关对100个作为action generator的输入；   这里有两步操作，1 选出哪些filter； 2 这些filter再去进行速度或动作等的预测，这里分别用1000个filter 去预测（seqgan的很多D的判断，那就变成了一步。），发现稳定的20个是最相关的？？





动作也可以是词向量方式。几个类比。

1 densenet video2vide
2 readvideoA + 


3 模仿学习。
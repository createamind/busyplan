
-----------------------------
Plan   正式的：
记 反思； 最重要！  明确的！


更好智能：1更多的维度-加时间监督 videogan；2更多传感器，视听闻触等。
cardemo-autoware；
vide GAN; gan车道线，pix2pix;  深度到seg；3d；

传感器工控机  

5年  1年  1月 1周 今天  现在  今天最重要的目标计划：？？ 

采访的ppt！！盛大商业计划书，车的测试场地开出去。  机械臂


两个推进思路： 1  自动驾驶真车使用的需求   2 通用智能的构建思路推进。


-----------------------------
即时思考记录
10.15  16
深度：室外，室内； 双目视差、雷达、物体的摆放前后逻辑-视线阻挡。
cyclegan视频，单帧or 连续 ； 深度以外的cyclegan学习。  
cyclegan是成对的学习，或成对的数据集，pix2pix是成对的数据；能否不用成对数据集，直接从数据集中学习各种属性，遇到其他数据集继续类似cyclegan学习对应数据集的对应属性变化？？
或图灵机自己选择某些数据组成成对的数据集。

语言到图像：

pix2pix  到 sound2video  （have text to image 这个的相关介绍 ） 
Ambient Sound Provides Supervision for Visual Learning  https://arxiv.org/pdf/1608.07017.pdf

深度到rgb： Self-Paced Cross-Modality Transfer Learning for Efficient Road Segmentation. In ICRA2017


video 视频的context信息，声音和图像的对应关系也有。 ego-motion

通用智能的构建思路： 1 运动（参考生物视觉发展），  videgan(avb?)     1.1 我不动，周围运动，1.2自我运动的检测。   












--------------------------------------------------
智能架构思路：

方向：
1 哪些需要我们自己去突破（我们有积累和思考，有自己的思路方向）:  更好的模型，cyclegan；
2 哪些是我们去跟进即可（别人比我们更专业更有资源和实力），比如apollo，比如硬件改装车，比如px2
精力注意力首先放到要突破的工作去。



todo： 
招人：之前跑的成果展示

指导思想：集成已有算法代码而非自己研究算法代码。
集成：整个系统的集成：各个子功能的实现。

无监督技术调研：

模型：
1  pix2pix应用到多传感器、多场景、每个单词都是一个conditon
之前的pix2pix 文章：
最近比较火的三个GAN应用及代码--Pix2pix
论文 pix2pix  最近比较火应用背后的技术思想
cyclegan、discogan
CycleGAN: pix2pix without input-output pairs
Discover Cross-Domain Relations with GAN



多传感器：
为什么从视觉里我们看到石头和苹果就能体会到他们的不同的硬度和口味？
看到小狗和麻雀为什么会有不同的联想。铁丝和粉丝感觉更不同。
视觉看到和我们体会物体的属性是有关联的，那么如何进行这种关联呢？
如何训练能联想的模型。
segmentation的本质是不同物体的有核心不同的属性，这些不同的属性从哪里来？从其他传感器维度来，视觉图像和这些属性建立了关联、联觉、联想最终我们通过视觉就能进行这种属性的区分
pix2 pix   多传感器 互为 不同方面进行互相生成。视觉对热感受的识别；通感训练。通感视觉对其他属性的感知，
视觉和其他很多物体的属性都有关联：视觉与听觉，视觉与触觉（软硬、凉热等属性）嗅觉（味道属性等）
及不同属性所需要的不同的注意力：活动的蛇与静止的蛇。
部分文字就是对这种属性的标注link等调用。


多条件condition：
pix2pix 应用到自身运动估计，摄像头移动或静止。
condition：我走，环境是这样变化，condition：我静止，环境其他物体的运动应该是怎么样的。学习背景与物体（videogan 周末轻松一刻，欣赏完全由程序自己回忆的视频片段）。


摄像头，深度双目摄像头，雷达，能不能学出来独立物体？？从不同的深度区分，不同的属性区分（雷达测试密度）
视觉和雷达点云深度信息的互相学习。视觉从雷达探测的物体属性进行学习。
学习不同场景、不同物体、不同大小、不同特点、不同物体展示的深度信息。


视觉与深度、雷达  rgbd； 与imu；移动速度的估计，与红外，热量、声呐；等等其他传感器。



测试：任务分解细化：


这个和sfm-net有类似地方，sfm是手动设置loss函数。
以上全部在一个大网络里面训练，或如何自动拆分为小网络，或网络如何自动扩展、扩容。



-1  betavae 单特征连续变化学习

通用智能架构：

11  运动的监督，视频的监督，时间的监督-------就是视频里的学习

2  光流论文代码(监督训练)和sfm-net论文（无监督）

1 光流检测运动   ： 1 自动标注的作用  https://github.com/msracver/Deep-Feature-Flow
光流对segmentation分割的学习可以结合fasterrcnn对区域合并的网络训练。
视频和光流对的pix2pix。
视频和segmentation的pix2pix
视频里运动物体的预测感知。


2 sfm
最近的无监督学习： sfm learning   自我运动学习

Unsupervised Learning of Depth and Ego-Motion from Video  及代码
sfmlearning   https://github.com/tinghuiz/SfMLearner
https://github.com/mrharicot/monodepth  场景深度学习


TCN: 端到端机器人自学习算法
3 time contrast 
Time-Contrastive Networks: Self-Supervised Learning from Multi-View Observation


光流做分割的数据准备—-论文 光流视频 一篇论文！！https://github.com/msracver/Deep-Feature-Flow

slam—pix2pix 生成卫星图的道路位置！  细节信息如何用生成模型来预测！！？？

4 ros 多机器，小车采集数据到服务器训练，实时或bag文件保存

5 ros传感器类型数据： 视频，雷达3d，红外、声呐、imu，的数据保存；
5.2开放数据的使用进行 pix2pix训练！包括kitti，滴滴、百度，udacity等数据的训练。

6 增强学习

7 图灵机模型   最新deepmind时间生成


自动驾驶技术：305篇论文调研；

视觉论文总结的文章；视觉的大厚书  
生物视觉了解：
论文解读：主视觉大脑皮层的深度层级模型：机器视觉可以从中学到些什么？

数据集：kittiudaciytbaidu  awesome

参考自动驾驶第一本书---相关的12篇文章


版本升级测试：应该是智能的测试，类似考题了，语言交互进行测试（1 语言学习及语言接口）。






--------------------------------------------------
语义学习思路：

​

一：概述
二：通俗介绍
三：技术相关模块资源



一：概述：

《思想本质》一书通过语言和认知对人的思想认知等进行了分析，前部分有一个核心观点是(李德毅院士也提过)：语言是思想认知体系的语义索引，语言只是符号，语言表达的含义即语言背后的认知体系是决定语言的根本。任何一个具体的词语，比如桌子，‘桌子’是两个字，可以翻译为其他语言，但是桌子这个概念代表了一种家庭工具，通常是木制的用来放物品的一个工具，这个工具是看得到，有总量，有形状的一个具体物体，这个桌子概念代表了一种具体的认知，‘桌子’这个词本身只是一个词，背后是一个认知概念体系。


语言是语义索引，背后有一个很大的认知体系为基础，认知基础之一来自视觉对现实世界的认识，物理规律，场景的学习理解，跨越长时间的事件的记忆理解及社会互动理解等等。但是需要从基础的基本的语义概念开始学习和实现。

人思考以概念为基础单位，每个概念都是一个语义认知的单位，人智慧高于其他动物就是因为人的抽象思维层次更高，思维活动是在语义认知层面进行思维。所以现在如果做通用人工智能应该从视觉开始，语言其次；首先通过视觉对外部世界达到语义认识，然后在语义认识的基础之上进行思维，进行其他认识活动的构建。

deepmind也有类似思路，见deepmind 做通用人工智能的思路，及理论 | 暑期课程最后一讲：理论神经科学和深度学习理论，且现在的深度网络已经可以学习语义特征，只是技术还非常原始初级。ref   beta-vae  超越 infogan的无监督学习框架--效果更新   ！    使用infogan学习可解释的隐变量特征学习-及代码示例（代码和官方有差异）

上面链接展示的语义认知，还仅限于静态图像，视频中跨时间维度的如动作（运动信息向量的神经网络学习 code、ppt、视频ok）等事件抽象 (还没有做到)。





感知作为外部信息输入的第一步，作用是抽取核心语义特征属性。高层次的逻辑判断基于底层的基本语义单位的认知元素。比如翻译就是对同一事件的不用语言描述，事件是唯一认知，描述可以用不同语言描述，不同语言的描述都是基于同一事件的多个元素在不同语言里对应的概念名称进行描述。
思考运算基于物体的核心属性推理 ref deepmind 做通用人工智能的思路

拿驾驶来说
驾驶的核心是认路，路的概念是核心，路的方向是驾驶的根本。
路的一个属性是路的延伸方向，驾驶的核心既是控制车辆的前进的方向跟路的延伸方向相同即可，方向盘需要和路的方向一致，如果学习到路的方向这个抽象概念，那方向控制就太容易了，路的方向本质是路延伸的指向。如果能吧路抽象成一条线，而且是把各种不同的路（高速路，山路，街道）统一抽象能很好的泛化，方向盘的控制就解决了。


深度学习的动机与挑战之-流形学习 提到道路就是嵌入在三维空间中的一维流形。
GAN对流形的优化提高稳定性：
理论|来聊聊最近很火的WGAN
通俗|令人拍案叫绝的Wasserstein GAN 及代码（WGAN两篇论文的中文详细介绍）


刹车控制的基础是危险的概念，危险的识别和认知就比较复杂了。要认识这个世界的危险，先认识这个世界，认识就是找出事物发展的规律，需要先总结多个事件，至少需要先能描述多个事情，先分门别类的区分多个事件，多个到一个，先能描述一个事件，先重现一个事件，落实到具体技术就是：gan生成视频

认识需要 1 区分不同的属性，是否危险（学习独立事物，独立个体，独立场景，个体的颜色属性，声音属性，触觉属性，气温属性），什么条件能更方便找到食物。什么环境下的什么特征？颜色特征，天气特征（这个太大，天气的其他属性），这些属性会通过drl强化学习关联到reward，出现部分动物迷信的特点。部分动物或鸟类只根据某些颜色等特征进行固定的行为。其实条件反射也类似。更细致的学习这些特征就类似GAN学习到眼镜、笑脸、男性、女性等概念一样。

另外物体跟踪算法可以为物体的识别提供大量样本进行训练，视觉中连续的物体 https://github.com/msracver/Deep-Feature-Flow 


不同场景路对应不同的概念，比如城市中的路概念对应"街道"。街道的概念其实有很多的相关概念：行人、建筑、交通标识、路口等等。为了理解街道的概念需要以上相关概念的学习。当然概念的学习是一个渐进的过程，概念可以分化，细化。

深度学习学习到的概念如果能类似人实时改进学习提高，且不同的概念能及时学习提高，快速适应环境变化，这样才能有更好的适应能力。
考虑概念的层级组合关系，子概念的概念学习可以在不影响架构的情况下，直接提高系统的认知能力。

传统自动驾驶软件如果将各种条件提前做好配置，则失去了不同环境的适应能力。


现存技术障碍：语义学习技术处于非常初级阶段，模型的稳定性，扩展性，训练模型结合其他功能的协调，如和动作控制进行结合，属性的相关操作比如手臂的协调控制指示等，注意力等。

语义学习可以从beta VAE入手  ref 谷歌：beta-vae 可以媲美infogan的无监督学习框架-多图-及代码；

#对deepmind强化学习与通用智能的个人理解；强化学习对好坏利弊的追求适用于任何智能层次的主体；即适用于任何层次的生物，但是类人智能需要在认知上有更高的抽象，所以更好的无监督抽象学习将提高强化学习的效果（比如抽象学习到路的方向）。所以强化学习使用学习到更抽象概念的的GAN模型进行特征提取的辅助能更高效。


二：通俗介绍
为什么视觉重要：
这个链接有丰富的内容：论文解读：主视觉大脑皮层的深度层级模型：机器视觉可以从中学到些什么？

视觉是动物对世界认识极其重要的一个入口，通过视觉，动物对世界进行认识，把世界在自己的大脑中进行建模。
视觉无论从生物的进化，还是从个体生物的发育来看，视觉都是基础，其他的思维很大比例都是构建在视觉基础之上，很多语言都是首先反映普通的视觉对象，抽象词语也大都是从具体的词语抽象而来
智慧的高低是抽象思维的深度，是理解概念的抽象深度，概念最基础的应该是具体的物体的概念，视觉层次的物体形体概念，
人比其他动物更强的是大脑的其他高级功能更多更抽象，视觉神经系统相比其他感觉系统在整个大脑中所占比重也更大。

实现方法的深刻认识：才是改变未来的能力！向大自然已经实现的智能学习：动物的认知神经发育发展过程，这样实现过程才是有一定保证的。

在与世界的互动水平上面反映了地球生物的智能水平，对这个世界认识越深刻那么这个生物越智能。人作为这个世界最高智慧生物，学习人的认知发展不会导致类人智能的研发出现很大的偏差。
想更智能就要对世界有更深入的认识，想真正的认识世界，需要从最底层开始认识这个世界，从最底层开始构建对世界的认知，像一个新生婴儿，甚至更早的小婴儿一样开始认识世界，积累对世界的认识
反观生物对世界认知的发展进化，发育演化，视觉是认识世界的重要基础。

视觉是动物对世界认识极其重要的一个入口，通过视觉，动物对世界进行认识，把世界在自己的大脑中进行建模。视觉神经系统在整个大脑中所占比重很大。
视觉无论从生物的进化，还是从个体生物的发育来看，视觉都是基础（视觉在人的成长中的作用，在人成长中对人智力的发展作用，视觉不好对人的智力影响之大不容置疑。的确生物中高级动物都是视觉认知为主。），其他的思维很大比例都是构建在视觉基础之上，很多语言都是首先反映普通的视觉对象，抽象词语也大都是从具体的词语抽象而来（立规矩的立就是从具体物体的站立而引申到抽象概念规矩的站立。规矩规则这种抽象概念是看不到的，但是能感受到，能理解它的存在，所以立规矩的立就从具体站立的概念延伸到了抽象领域。）
智慧的高低是抽象思维的深度，是理解概念的抽象深度，最基础的概念是具体的物体的概念，视觉中具体物体的形体概念，




三：技术模块资源：

-2 3d数据集有图像和深度！

-1 commaai 的gan 预测 ，

0    摄像头输入图像视频的WGAN训练

1    从视频中学习运动信息，
        运动信息向量的神经网络学习 code、ppt、视频ok
        周末轻松一刻，欣赏完全由程序自己回忆的视频片段
！videogan code

2    区分背景与主体；
        周末轻松一刻，欣赏完全由程序自己回忆的视频片段 自动学习区分主题背景及运动
        attribute-disentangled-VAE手动设置主体与背景等论文有涉及。

3    GAN的Dis可以是非常多的子网络分别对整体和局部进行监督学习-需用到2从运动中学到的主体。 
        wayaai有部分 pyramid层级生成；分类识别网络的多层级集成-每个概念的识别才有对应的每个概念的生成。
        https://github.com/255BITS/HyperGAN


4    从注意力机制引导学习焦点到特定的主体的局部。结合3自概念的学习

5    学习到的概念的语义标注即语言联系
        通过文字生成图片的神经网络技术
        文字生成图片的相关，训练过程应该先由简单物体学习再到复杂场景的学习。
语义标注betaVae INFOGan。

6    训练过程的多类别loss函数训练：比如先WGAN训练稳定然后beta-vae训练；不同维度互为监督-视觉听觉信息的互相监督学习：猫狗的叫声和形态听觉视觉互相监督学习。pix2pix  cycleGAN 等。
cyclegan 学习深度信息的效果  相当于从图像联想到深度，使用记忆联想，陌生环境才需要更实时的雷达深度探测，熟悉环境使用记忆提高效率（运算效率，使用效率）

！vae算法和WGAN算法的深入比较。
！wgan 输出二维是否会类似vae？
！lsgan效果测试
    
7    模型扩展技术-子结构学习比如人脸的眼镜-鼻子；子结构的自动学习

8    结合强化学习的特征利用，强化学习使用betaVAE特征。强化学习验证智能学习的水平程度。deepdrive自动驾驶的架构，
！强化学习代码 开源强化学习程序架构

9    复合模型，比如yolo9000中使用了很多技巧；模型整合；代码阅读分解。

10    时间序列学习；神经图灵机；GTMM；

11 自监督特征学习： https://mp.weixin.qq.com/s/DOxsLm3bYTNp3-uke0dVTA

12 apollo


end.





 对视觉概念学习有兴趣的可以联系我微信zdx3578；



         关注公众号获取更多内容：
